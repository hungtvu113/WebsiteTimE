name: ðŸ“Š Code Quality & Performance

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Cháº¡y hÃ ng ngÃ y lÃºc 2:00 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:

env:
  NODE_VERSION: '18'
  PNPM_VERSION: '8.15.0'

jobs:
  # ðŸ“Š SonarCloud Analysis
  sonarcloud:
    name: ðŸ“Š SonarCloud Analysis
    runs-on: ubuntu-latest
    if: github.event_name != 'schedule' || github.ref == 'refs/heads/main'
    services:
      mongodb:
        image: mongo:6.0
        env:
          MONGO_INITDB_ROOT_USERNAME: admin
          MONGO_INITDB_ROOT_PASSWORD: password123
          MONGO_INITDB_DATABASE: qltime_test
        ports:
          - 27017:27017
        options: >-
          --health-cmd "mongosh --eval 'db.adminCommand(\"ping\")'"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    steps:
      - name: ðŸ“¥ Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Shallow clones should be disabled for better analysis

      - name: ðŸ“¦ Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: ðŸŸ¢ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'pnpm'

      - name: ðŸ“¥ Install dependencies
        run: pnpm install --frozen-lockfile

      - name: ðŸ§ª Run tests with coverage
        run: |
          pnpm --filter backend test:cov
        env:
          MONGODB_URI: mongodb://admin:password123@localhost:27017/qltime_test?authSource=admin
          JWT_SECRET: test_jwt_secret
          NODE_ENV: test

      - name: ðŸ“Š SonarCloud Scan
        uses: SonarSource/sonarqube-scan-action@v5.0.0
        continue-on-error: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
          SONAR_HOST_URL: https://sonarcloud.io

      - name: ðŸ“Š SonarCloud Quality Gate
        uses: SonarSource/sonarqube-quality-gate-action@master
        continue-on-error: true
        timeout-minutes: 5
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}

      - name: âš ï¸ SonarCloud Status
        run: |
          if [ -z "${{ secrets.SONAR_TOKEN }}" ]; then
            echo "âš ï¸ SonarCloud analysis skipped - SONAR_TOKEN not configured"
            echo "To enable SonarCloud:"
            echo "1. Go to https://sonarcloud.io"
            echo "2. Import this repository"
            echo "3. Add SONAR_TOKEN to GitHub secrets"
          else
            echo "âœ… SonarCloud analysis completed (check steps above for results)"
          fi

  # âš¡ Performance Testing
  performance:
    name: âš¡ Performance Testing
    runs-on: ubuntu-latest
    services:
      mongodb:
        image: mongo:6.0
        env:
          MONGO_INITDB_ROOT_USERNAME: admin
          MONGO_INITDB_ROOT_PASSWORD: password123
          MONGO_INITDB_DATABASE: qltime_test
        ports:
          - 27017:27017
        options: >-
          --health-cmd "mongosh --eval 'db.adminCommand(\"ping\")'"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: ðŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ðŸ“¦ Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: ðŸŸ¢ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'pnpm'

      - name: ðŸ“¥ Install dependencies
        run: pnpm install --frozen-lockfile

      - name: ðŸš€ Start backend for testing
        run: |
          pnpm backend:build
          pnpm backend:start &
          echo $! > backend.pid
          
          # Wait for backend to start
          for i in {1..30}; do
            if curl -f http://localhost:3001/health > /dev/null 2>&1; then
              echo "âœ… Backend is ready"
              break
            fi
            echo "â³ Waiting for backend... ($i/30)"
            sleep 2
          done
        env:
          MONGODB_URI: mongodb://admin:password123@localhost:27017/qltime_test?authSource=admin
          JWT_SECRET: test_jwt_secret
          NODE_ENV: production
          PORT: 3001

      - name: ðŸ“¦ Install Artillery and dependencies
        run: |
          npm install -g artillery@latest
          # Install bc for calculations
          sudo apt-get update && sudo apt-get install -y bc jq

      - name: âš¡ Run API performance tests
        run: |
          echo "âš¡ Running API performance tests..."

          # Create performance test config if not exists
          if [ ! -f "performance-test.yml" ]; then
            cat > performance-test.yml << EOF
          config:
            target: 'http://localhost:3001'
            phases:
              - duration: 30
                arrivalRate: 5
                name: "Warm up"
              - duration: 60
                arrivalRate: 10
                name: "Load test"
              - duration: 30
                arrivalRate: 15
                name: "Stress test"

          scenarios:
            - name: "Health check"
              weight: 50
              flow:
                - get:
                    url: "/health"

            - name: "API endpoints"
              weight: 50
              flow:
                - get:
                    url: "/api"
                    expect:
                      - statusCode: 200
          EOF
          fi

          # Run performance tests vÃ  táº¡o files ngay cáº£ khi fail
          set +e
          artillery run performance-test.yml --output performance-results.json || echo "Performance test completed with warnings"
          set -e

          # Táº¡o file results náº¿u khÃ´ng tá»“n táº¡i
          if [ ! -f "performance-results.json" ]; then
            echo '{"aggregate":{"counters":{"http.requests":0},"latency":{"mean":0,"p95":0,"p99":0}}}' > performance-results.json
          fi

          # Generate HTML report
          artillery report performance-results.json --output performance-report.html || echo "Report generation completed"

      - name: ðŸ“Š Analyze performance results
        run: |
          echo "ðŸ“Š Analyzing performance results..."

          # Extract key metrics
          if [ -f "performance-results.json" ]; then
            echo "ðŸ“ˆ Performance Summary:"
            echo "- Total requests: $(jq '.aggregate.counters["http.requests"] // 0' performance-results.json)"
            echo "- Success rate: $(jq '.aggregate.counters["http.responses"] // 0' performance-results.json)"
            echo "- Average response time: $(jq '.aggregate.latency.mean // 0' performance-results.json)ms"
            echo "- 95th percentile: $(jq '.aggregate.latency.p95 // 0' performance-results.json)ms"
            echo "- 99th percentile: $(jq '.aggregate.latency.p99 // 0' performance-results.json)ms"

            # Check if performance is acceptable (only if we have valid data)
            AVG_RESPONSE=$(jq '.aggregate.latency.mean // 0' performance-results.json)
            P95_RESPONSE=$(jq '.aggregate.latency.p95 // 0' performance-results.json)

            # Only check if we have meaningful data
            if [ "$AVG_RESPONSE" != "0" ] && [ "$AVG_RESPONSE" != "null" ]; then
              if (( $(echo "$AVG_RESPONSE > 1000" | bc -l 2>/dev/null || echo 0) )); then
                echo "âš ï¸ Average response time is high: ${AVG_RESPONSE}ms"
              else
                echo "âœ… Average response time is acceptable: ${AVG_RESPONSE}ms"
              fi
            fi

            if [ "$P95_RESPONSE" != "0" ] && [ "$P95_RESPONSE" != "null" ]; then
              if (( $(echo "$P95_RESPONSE > 2000" | bc -l 2>/dev/null || echo 0) )); then
                echo "âš ï¸ 95th percentile response time is high: ${P95_RESPONSE}ms"
              else
                echo "âœ… 95th percentile response time is acceptable: ${P95_RESPONSE}ms"
              fi
            fi
          else
            echo "âš ï¸ Performance results file not found, creating empty report"
            echo '{"message": "Performance test was skipped or failed"}' > performance-results.json
          fi

      - name: ðŸ›‘ Stop backend
        if: always()
        run: |
          if [ -f backend.pid ]; then
            kill $(cat backend.pid) || true
            rm backend.pid
          fi

      - name: ðŸ“¦ Upload performance results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-results
          path: |
            performance-results.json
            performance-report.html
            performance-test.yml
          retention-days: 30
          if-no-files-found: warn

  # ðŸŒ Frontend Lighthouse Audit
  lighthouse:
    name: ðŸŒ Lighthouse Audit
    runs-on: ubuntu-latest
    steps:
      - name: ðŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ðŸ“¦ Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: ðŸŸ¢ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'pnpm'

      - name: ðŸ“¥ Install dependencies
        run: pnpm install --frozen-lockfile

      - name: ðŸ—ï¸ Build frontend
        run: pnpm frontend:build
        env:
          NEXT_PUBLIC_API_URL: http://localhost:3001

      - name: ðŸš€ Start frontend
        run: |
          pnpm frontend:start &
          echo $! > frontend.pid
          
          # Wait for frontend to start
          for i in {1..30}; do
            if curl -f http://localhost:3000 > /dev/null 2>&1; then
              echo "âœ… Frontend is ready"
              break
            fi
            echo "â³ Waiting for frontend... ($i/30)"
            sleep 2
          done

      - name: ðŸŒ Run Lighthouse CI
        run: |
          npm install -g @lhci/cli@latest
          
          # Create Lighthouse CI config if not exists
          if [ ! -f ".lighthouserc.json" ]; then
            cat > .lighthouserc.json << EOF
          {
            "ci": {
              "collect": {
                "url": ["http://localhost:3000"],
                "numberOfRuns": 3
              },
              "assert": {
                "assertions": {
                  "categories:performance": ["warn", {"minScore": 0.8}],
                  "categories:accessibility": ["error", {"minScore": 0.9}],
                  "categories:best-practices": ["warn", {"minScore": 0.8}],
                  "categories:seo": ["warn", {"minScore": 0.8}]
                }
              },
              "upload": {
                "target": "temporary-public-storage"
              }
            }
          }
          EOF
          fi
          
          lhci autorun

      - name: ðŸ›‘ Stop frontend
        if: always()
        run: |
          if [ -f frontend.pid ]; then
            kill $(cat frontend.pid) || true
            rm frontend.pid
          fi

  # ðŸ”’ Security Analysis
  security:
    name: ðŸ”’ Security Analysis
    runs-on: ubuntu-latest
    permissions:
      actions: read
      contents: read
      security-events: write
    steps:
      - name: ðŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ðŸ” Initialize CodeQL
        uses: github/codeql-action/init@v3
        with:
          languages: javascript

      - name: ðŸ” Perform CodeQL Analysis
        uses: github/codeql-action/analyze@v3

      - name: ðŸ”’ Run Semgrep
        uses: returntocorp/semgrep-action@v1
        with:
          config: >-
            p/security-audit
            p/secrets
            p/javascript
            p/typescript
        env:
          SEMGREP_APP_TOKEN: ${{ secrets.SEMGREP_APP_TOKEN }}

  # ðŸ“š Documentation Check
  docs:
    name: ðŸ“š Documentation Check
    runs-on: ubuntu-latest
    steps:
      - name: ðŸ“¥ Checkout code
        uses: actions/checkout@v4

      - name: ðŸ“š Check README files
        run: |
          echo "ðŸ“š Checking documentation..."
          
          # Check if main README exists and is not empty
          if [ ! -f "README.md" ] || [ ! -s "README.md" ]; then
            echo "âŒ Main README.md is missing or empty"
            exit 1
          fi
          
          # Check for other important docs
          DOCS_TO_CHECK=("frontend/README.md" "backend/README.md")
          
          for doc in "${DOCS_TO_CHECK[@]}"; do
            if [ ! -f "$doc" ]; then
              echo "âš ï¸ Missing documentation: $doc"
            else
              echo "âœ… Found: $doc"
            fi
          done

      - name: ðŸ“ Check for TODO comments
        run: |
          echo "ðŸ“ Checking for TODO comments..."
          
          TODO_COUNT=$(grep -r "TODO\|FIXME\|HACK" --include="*.ts" --include="*.tsx" --include="*.js" --include="*.jsx" . | wc -l)
          
          echo "ðŸ“ Found $TODO_COUNT TODO/FIXME/HACK comments"
          
          if [ "$TODO_COUNT" -gt 50 ]; then
            echo "âš ï¸ High number of TODO comments found. Consider addressing some of them."
          fi

      - name: ðŸ“Š Generate documentation report
        run: |
          echo "ðŸ“Š Generating documentation report..."
          
          cat > docs-report.md << EOF
          # ðŸ“š Documentation Report
          
          Generated on: $(date)
          
          ## ðŸ“‹ Documentation Status
          - Main README: $([ -f "README.md" ] && echo "âœ… Present" || echo "âŒ Missing")
          - Frontend README: $([ -f "frontend/README.md" ] && echo "âœ… Present" || echo "âŒ Missing")
          - Backend README: $([ -f "backend/README.md" ] && echo "âœ… Present" || echo "âŒ Missing")
          - Docker README: $([ -f "README-DOCKER.md" ] && echo "âœ… Present" || echo "âŒ Missing")
          
          ## ðŸ“ Code Comments
          - TODO comments: $(grep -r "TODO" --include="*.ts" --include="*.tsx" --include="*.js" --include="*.jsx" . | wc -l)
          - FIXME comments: $(grep -r "FIXME" --include="*.ts" --include="*.tsx" --include="*.js" --include="*.jsx" . | wc -l)
          - HACK comments: $(grep -r "HACK" --include="*.ts" --include="*.tsx" --include="*.js" --include="*.jsx" . | wc -l)
          EOF

      - name: ðŸ“¦ Upload documentation report
        uses: actions/upload-artifact@v4
        with:
          name: documentation-report
          path: docs-report.md
          retention-days: 30
